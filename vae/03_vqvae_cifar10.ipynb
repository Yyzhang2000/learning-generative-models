{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMZvmtuhnyX8wBjatUsKjzN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yyzhang2000/learning-generative-models/blob/main/vae/03_vqvae_cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPWfhjw8n8e0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "from torchvision.utils import save_image, make_grid\n",
        "\n",
        "from tqdm.autonotebook import trange\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = \"./data\"\n",
        "batch_size = 128\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "NGMBEkPJolGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autoencoder Model"
      ],
      "metadata": {
        "id": "OjHq1rO4owJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VQ-VAE Model"
      ],
      "metadata": {
        "id": "-3ja_NXfVvAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    in_channels: int = 3\n",
        "    num_hiddens: int = 128\n",
        "    num_downsampling_layers: int = 2\n",
        "    num_residual_layers: int = 2\n",
        "    num_residual_hiddens: int = 32\n",
        "    embedding_dim: int = 64\n",
        "    num_embeddings: int = 512\n",
        "    use_ema: bool = True\n",
        "    decay: float = 0.99\n",
        "    epsilon: float = 1e-5"
      ],
      "metadata": {
        "id": "j-iS_NrNhxBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualStack(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            num_hiddens,\n",
        "            num_residual_layers,\n",
        "            num_residual_hiddens\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        for i in range(num_residual_layers):\n",
        "            layers.append(\n",
        "                nn.Sequential(nn.ReLU(),\n",
        "                              nn.Conv2d(num_hiddens, num_residual_hiddens, kernel_size=3, padding =1 ),\n",
        "                              nn.ReLU(),\n",
        "                              nn.Conv2d(num_residual_hiddens, num_hiddens, kernel_size = 1)\n",
        "                              )\n",
        "            )\n",
        "\n",
        "        self.layers = nn.ModuleList(*layers)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x\n",
        "        for layer in self.layers:\n",
        "            h = h + layer(h)\n",
        "\n",
        "            return torch.relu(h)"
      ],
      "metadata": {
        "id": "MpBY-ILGVyjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder"
      ],
      "metadata": {
        "id": "lEZAkQNyVxJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            num_hiddens,\n",
        "            num_downsampling_layers,\n",
        "            num_residual_layers,\n",
        "            num_residual_hiddens\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        conv = nn.Sequential()\n",
        "        for downsampling_layer in range(num_downsampling_layers):\n",
        "            if downsampling_layer == 0:\n",
        "                out_channels = num_hiddens // 2\n",
        "            elif downsampling_layer == 1:\n",
        "                (in_channels, out_channels) = (num_hiddens // 2, num_hiddens)\n",
        "            else:\n",
        "                (in_channels, out_channels) = (num_hiddens, num_hiddens)\n",
        "\n",
        "            conv.add_module(\n",
        "                f\"down{downsampling_layer}\",\n",
        "                nn.Conv2d(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=out_channels,\n",
        "                    kernel_size=4,\n",
        "                    stride=2,\n",
        "                    padding=1,\n",
        "                ),\n",
        "            )\n",
        "            conv.add_module(f\"relu{downsampling_layer}\", nn.ReLU())\n",
        "\n",
        "        conv.add_module(\n",
        "            \"final_conv\",\n",
        "            nn.Conv2d(\n",
        "                in_channels=num_hiddens,\n",
        "                out_channels=num_hiddens,\n",
        "                kernel_size=3,\n",
        "                padding=1,\n",
        "            ),\n",
        "        )\n",
        "        self.conv = conv\n",
        "        self.residual_stack = ResidualStack(\n",
        "            num_hiddens, num_residual_layers, num_residual_hiddens\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.conv(x)\n",
        "        return self.residual_stack(h)"
      ],
      "metadata": {
        "id": "1vn3x8TOCbi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder"
      ],
      "metadata": {
        "id": "lHB7EeUwXHiU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim,\n",
        "        num_hiddens,\n",
        "        num_upsampling_layers,\n",
        "        num_residual_layers,\n",
        "        num_residual_hiddens,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # See Section 4.1 of \"Neural Discrete Representation Learning\".\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels=embedding_dim,\n",
        "            out_channels=num_hiddens,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "        )\n",
        "        self.residual_stack = ResidualStack(\n",
        "            num_hiddens, num_residual_layers, num_residual_hiddens\n",
        "        )\n",
        "        upconv = nn.Sequential()\n",
        "        for upsampling_layer in range(num_upsampling_layers):\n",
        "            if upsampling_layer < num_upsampling_layers - 2:\n",
        "                (in_channels, out_channels) = (num_hiddens, num_hiddens)\n",
        "\n",
        "            elif upsampling_layer == num_upsampling_layers - 2:\n",
        "                (in_channels, out_channels) = (num_hiddens, num_hiddens // 2)\n",
        "\n",
        "            else:\n",
        "                (in_channels, out_channels) = (num_hiddens // 2, 3)\n",
        "\n",
        "            upconv.add_module(\n",
        "                f\"up{upsampling_layer}\",\n",
        "                nn.ConvTranspose2d(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=out_channels,\n",
        "                    kernel_size=4,\n",
        "                    stride=2,\n",
        "                    padding=1,\n",
        "                ),\n",
        "            )\n",
        "            if upsampling_layer < num_upsampling_layers - 1:\n",
        "                upconv.add_module(f\"relu{upsampling_layer}\", nn.ReLU())\n",
        "\n",
        "        self.upconv = upconv\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.conv(x)\n",
        "        h = self.residual_stack(h)\n",
        "        x_recon = self.upconv(h)\n",
        "        return x_recon"
      ],
      "metadata": {
        "id": "qeoudcMXXHVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vecrtor Quantizer"
      ],
      "metadata": {
        "id": "aNmw5I0oXNVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SonnetExponentialMovingAverage(nn.Module):\n",
        "    # See: https://github.com/deepmind/sonnet/blob/5cbfdc356962d9b6198d5b63f0826a80acfdf35b/sonnet/src/moving_averages.py#L25.\n",
        "    # They do *not* use the exponential moving average updates described in Appendix A.1\n",
        "    # of \"Neural Discrete Representation Learning\".\n",
        "    def __init__(self, decay, shape):\n",
        "        super().__init__()\n",
        "        self.decay = decay\n",
        "        self.counter = 0\n",
        "        self.register_buffer(\"hidden\", torch.zeros(*shape))\n",
        "        self.register_buffer(\"average\", torch.zeros(*shape))\n",
        "\n",
        "    def update(self, value):\n",
        "        self.counter += 1\n",
        "        with torch.no_grad():\n",
        "            self.hidden -= (self.hidden - value) * (1 - self.decay)\n",
        "            self.average = self.hidden / (1 - self.decay ** self.counter)\n",
        "\n",
        "    def __call__(self, value):\n",
        "        self.update(value)\n",
        "        return self.average"
      ],
      "metadata": {
        "id": "EE0A-lq7h384"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VectorQuantizer(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            embedding_dim,\n",
        "            num_embeddings,\n",
        "            use_ema,\n",
        "            decay,\n",
        "            epsilon\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.use_ema = use_ema\n",
        "\n",
        "        self.decay = decay\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        limit = 3 ** 0.5\n",
        "\n",
        "        self.e_i_ts = torch.FloatTensor(embedding_dim ,num_embeddings).uniform_(-limit, limit)\n",
        "        if use_ema:\n",
        "            self.register_buffer(\"e_i_ts\", e_i_ts)\n",
        "        else:\n",
        "            self.register_parameter(\"e_i_ts\", nn.Parameter(e_i_ts))\n",
        "\n",
        "        self.N_i_ts = SonnetExponentialMovingAverage(decay, (num_embeddings,))\n",
        "        # Exponential moving average of the embeddings.\n",
        "        self.m_i_ts = SonnetExponentialMovingAverage(decay, e_i_ts.shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        flat_x = x.permute(0, 2, 3, 1).reshape(-1, self.embedding_dim)\n",
        "        distances = (\n",
        "            (flat_x ** 2).sum(1, keepdim = True)\n",
        "            -\n",
        "            2 * flat_x @ self.e_i_ts\n",
        "            +\n",
        "            (self.e_i_ts ** 2).sum(0, keepdim = True)\n",
        "        )\n",
        "\n",
        "        encoding_indices = distances.argmin(1)\n",
        "\n",
        "        quantized_x = F.embedding(\n",
        "            encoding_indices.view(x.shape[0], *x.shape[2:]), self.e_i_ts.transpose(0,1)\n",
        "        ).permute(0, 3, 1, 2)\n",
        "\n",
        "        if not self.use_ema:\n",
        "            dictionary_loss = ((x.detach() - quantized_x) ** 2).mean()\n",
        "        else:\n",
        "            distionary_loss = None\n",
        "\n",
        "        commitment_loss = ((x - quantized_x.detach()) ** 2).mean()\n",
        "        quantized_x = x + (quantized_x - x).detach()\n",
        "\n",
        "        if self.use_ema and self.training:\n",
        "            with torch.no_grad():\n",
        "                encoding_one_hots"
      ],
      "metadata": {
        "id": "tWYlAZg9XGrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VQVAE(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            config\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(config)\n",
        "        self.pre_vq_conv = nn.Conv2d(in_channels = config.num_hiddens, out_channels = embedding_dim, kernel_size = 1)\n",
        "\n",
        "        self.vq = VectorQuantizer(config)\n",
        "\n",
        "        self.decoder = Decoder(config)\n",
        "\n",
        "    def quantize(self, x):\n",
        "        z = self.pre_vq_conv(self.encoder(x))\n",
        "        return self.vq(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        (z_quantized, dictionary_loss, commitment_loss, _) = self.quantize(x)\n",
        "        x_hat = self.decoder(z_quantized)\n",
        "\n",
        "        return {\n",
        "            \"dictionary_loss\": dictionary_loss,\n",
        "            \"commitment_loss\": commitment_loss,\n",
        "            \"x_recon\": x_recon,\n",
        "        }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "SFIgJ_e0f7KP",
        "outputId": "0014b49e-a627-42ed-e130-6faada3f78cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c3fd8968c12d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mVQVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     def __init__(\n\u001b[1;32m      3\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     ):\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "workers = 10\n",
        "normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[1.0, 1.0, 1.0])\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ]\n",
        ")\n",
        "data_root = \"../data\"\n",
        "train_dataset = CIFAR10(data_root, True, transform, download=True)\n",
        "train_data_variance = np.var(train_dataset.data / 255)\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=workers,\n",
        ")"
      ],
      "metadata": {
        "id": "PR6FqNUxggz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beta = 0.25"
      ],
      "metadata": {
        "id": "7KVKGeutgwZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_params = [params for params in model.parameters()]\n",
        "lr = 3e-4\n",
        "optimizer = Adam(train_params, lr = lr)\n",
        "criterion = nn.MSELoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "WEBq1zaMgzLp",
        "outputId": "27e70113-5546-4c57-fb86-83d92c9875d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-3dee48238e0b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3e-4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "model.train()\n",
        "losses = []\n",
        "for epoch in trange(epochs):\n",
        "    for imgs, _ in train_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        out = model(imgs)\n",
        "\n",
        "        recon_error = criterion(out[\"x_recon\"], imgs) / train_data_variance\n",
        "        loss = recon_error + beta * out[\"commitment_loss\"]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())"
      ],
      "metadata": {
        "id": "Bd6-ssffg9s3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_img_tensors_as_grid(img_tensors, nrows, f):\n",
        "    img_tensors = img_tensors.permute(0, 2, 3, 1)\n",
        "    imgs_array = img_tensors.detach().cpu().numpy()\n",
        "    imgs_array[imgs_array < -0.5] = -0.5\n",
        "    imgs_array[imgs_array > 0.5] = 0.5\n",
        "    imgs_array = 255 * (imgs_array + 0.5)\n",
        "    (batch_size, img_size) = img_tensors.shape[:2]\n",
        "    ncols = batch_size // nrows\n",
        "    img_arr = np.zeros((nrows * batch_size, ncols * batch_size, 3))\n",
        "    for idx in range(batch_size):\n",
        "        row_idx = idx // ncols\n",
        "        col_idx = idx % ncols\n",
        "        row_start = row_idx * img_size\n",
        "        row_end = row_start + img_size\n",
        "        col_start = col_idx * img_size\n",
        "        col_end = col_start + img_size\n",
        "        img_arr[row_start:row_end, col_start:col_end] = imgs_array[idx]\n",
        "\n",
        "    Image.fromarray(img_arr.astype(np.uint8), \"RGB\").save(f\"{f}.jpg\")"
      ],
      "metadata": {
        "id": "COMrkWEShokL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "valid_dataset = CIFAR10(data_root, False, transform, download=True)\n",
        "valid_loader = DataLoader(\n",
        "    dataset=valid_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=workers,\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    valid_tensors = next(iter(valid_loader))\n",
        "\n",
        "    save_img_tensors_as_grid(valid_tensors[0], 4, \"true\")\n",
        "    save_img_tensors_as_grid(model(valid_tensors[0].to(device))[\"x_recon\"], 4, \"recon\")"
      ],
      "metadata": {
        "id": "0aqJaWAyhcYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ported from: https://github.com/deepmind/sonnet/blob/v2/examples/vqvae_example.ipynb.\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class ResidualStack(nn.Module):\n",
        "    def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super().__init__()\n",
        "        # See Section 4.1 of \"Neural Discrete Representation Learning\".\n",
        "        layers = []\n",
        "        for i in range(num_residual_layers):\n",
        "            layers.append(\n",
        "                nn.Sequential(\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(\n",
        "                        in_channels=num_hiddens,\n",
        "                        out_channels=num_residual_hiddens,\n",
        "                        kernel_size=3,\n",
        "                        padding=1,\n",
        "                    ),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(\n",
        "                        in_channels=num_residual_hiddens,\n",
        "                        out_channels=num_hiddens,\n",
        "                        kernel_size=1,\n",
        "                    ),\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x\n",
        "        for layer in self.layers:\n",
        "            h = h + layer(h)\n",
        "\n",
        "        # ResNet V1-style.\n",
        "        return torch.relu(h)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        num_hiddens,\n",
        "        num_downsampling_layers,\n",
        "        num_residual_layers,\n",
        "        num_residual_hiddens,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # See Section 4.1 of \"Neural Discrete Representation Learning\".\n",
        "        # The last ReLU from the Sonnet example is omitted because ResidualStack starts\n",
        "        # off with a ReLU.\n",
        "        conv = nn.Sequential()\n",
        "        for downsampling_layer in range(num_downsampling_layers):\n",
        "            if downsampling_layer == 0:\n",
        "                out_channels = num_hiddens // 2\n",
        "            elif downsampling_layer == 1:\n",
        "                (in_channels, out_channels) = (num_hiddens // 2, num_hiddens)\n",
        "\n",
        "            else:\n",
        "                (in_channels, out_channels) = (num_hiddens, num_hiddens)\n",
        "\n",
        "            conv.add_module(\n",
        "                f\"down{downsampling_layer}\",\n",
        "                nn.Conv2d(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=out_channels,\n",
        "                    kernel_size=4,\n",
        "                    stride=2,\n",
        "                    padding=1,\n",
        "                ),\n",
        "            )\n",
        "            conv.add_module(f\"relu{downsampling_layer}\", nn.ReLU())\n",
        "\n",
        "        conv.add_module(\n",
        "            \"final_conv\",\n",
        "            nn.Conv2d(\n",
        "                in_channels=num_hiddens,\n",
        "                out_channels=num_hiddens,\n",
        "                kernel_size=3,\n",
        "                padding=1,\n",
        "            ),\n",
        "        )\n",
        "        self.conv = conv\n",
        "        self.residual_stack = ResidualStack(\n",
        "            num_hiddens, num_residual_layers, num_residual_hiddens\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.conv(x)\n",
        "        return self.residual_stack(h)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim,\n",
        "        num_hiddens,\n",
        "        num_upsampling_layers,\n",
        "        num_residual_layers,\n",
        "        num_residual_hiddens,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # See Section 4.1 of \"Neural Discrete Representation Learning\".\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels=embedding_dim,\n",
        "            out_channels=num_hiddens,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "        )\n",
        "        self.residual_stack = ResidualStack(\n",
        "            num_hiddens, num_residual_layers, num_residual_hiddens\n",
        "        )\n",
        "        upconv = nn.Sequential()\n",
        "        for upsampling_layer in range(num_upsampling_layers):\n",
        "            if upsampling_layer < num_upsampling_layers - 2:\n",
        "                (in_channels, out_channels) = (num_hiddens, num_hiddens)\n",
        "\n",
        "            elif upsampling_layer == num_upsampling_layers - 2:\n",
        "                (in_channels, out_channels) = (num_hiddens, num_hiddens // 2)\n",
        "\n",
        "            else:\n",
        "                (in_channels, out_channels) = (num_hiddens // 2, 3)\n",
        "\n",
        "            upconv.add_module(\n",
        "                f\"up{upsampling_layer}\",\n",
        "                nn.ConvTranspose2d(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=out_channels,\n",
        "                    kernel_size=4,\n",
        "                    stride=2,\n",
        "                    padding=1,\n",
        "                ),\n",
        "            )\n",
        "            if upsampling_layer < num_upsampling_layers - 1:\n",
        "                upconv.add_module(f\"relu{upsampling_layer}\", nn.ReLU())\n",
        "\n",
        "        self.upconv = upconv\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.conv(x)\n",
        "        h = self.residual_stack(h)\n",
        "        x_recon = self.upconv(h)\n",
        "        return x_recon\n",
        "\n",
        "\n",
        "class SonnetExponentialMovingAverage(nn.Module):\n",
        "    # See: https://github.com/deepmind/sonnet/blob/5cbfdc356962d9b6198d5b63f0826a80acfdf35b/sonnet/src/moving_averages.py#L25.\n",
        "    # They do *not* use the exponential moving average updates described in Appendix A.1\n",
        "    # of \"Neural Discrete Representation Learning\".\n",
        "    def __init__(self, decay, shape):\n",
        "        super().__init__()\n",
        "        self.decay = decay\n",
        "        self.counter = 0\n",
        "        self.register_buffer(\"hidden\", torch.zeros(*shape))\n",
        "        self.register_buffer(\"average\", torch.zeros(*shape))\n",
        "\n",
        "    def update(self, value):\n",
        "        self.counter += 1\n",
        "        with torch.no_grad():\n",
        "            self.hidden -= (self.hidden - value) * (1 - self.decay)\n",
        "            self.average = self.hidden / (1 - self.decay ** self.counter)\n",
        "\n",
        "    def __call__(self, value):\n",
        "        self.update(value)\n",
        "        return self.average\n",
        "\n",
        "\n",
        "class VectorQuantizer(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_embeddings, use_ema, decay, epsilon):\n",
        "        super().__init__()\n",
        "        # See Section 3 of \"Neural Discrete Representation Learning\" and:\n",
        "        # https://github.com/deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py#L142.\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.use_ema = use_ema\n",
        "        # Weight for the exponential moving average.\n",
        "        self.decay = decay\n",
        "        # Small constant to avoid numerical instability in embedding updates.\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        # Dictionary embeddings.\n",
        "        limit = 3 ** 0.5\n",
        "        e_i_ts = torch.FloatTensor(embedding_dim, num_embeddings).uniform_(\n",
        "            -limit, limit\n",
        "        )\n",
        "        if use_ema:\n",
        "            self.register_buffer(\"e_i_ts\", e_i_ts)\n",
        "        else:\n",
        "            self.register_parameter(\"e_i_ts\", nn.Parameter(e_i_ts))\n",
        "\n",
        "        # Exponential moving average of the cluster counts.\n",
        "        self.N_i_ts = SonnetExponentialMovingAverage(decay, (num_embeddings,))\n",
        "        # Exponential moving average of the embeddings.\n",
        "        self.m_i_ts = SonnetExponentialMovingAverage(decay, e_i_ts.shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        flat_x = x.permute(0, 2, 3, 1).reshape(-1, self.embedding_dim)\n",
        "        distances = (\n",
        "            (flat_x ** 2).sum(1, keepdim=True)\n",
        "            - 2 * flat_x @ self.e_i_ts\n",
        "            + (self.e_i_ts ** 2).sum(0, keepdim=True)\n",
        "        )\n",
        "        encoding_indices = distances.argmin(1)\n",
        "        quantized_x = F.embedding(\n",
        "            encoding_indices.view(x.shape[0], *x.shape[2:]), self.e_i_ts.transpose(0, 1)\n",
        "        ).permute(0, 3, 1, 2)\n",
        "\n",
        "        # See second term of Equation (3).\n",
        "        if not self.use_ema:\n",
        "            dictionary_loss = ((x.detach() - quantized_x) ** 2).mean()\n",
        "        else:\n",
        "            dictionary_loss = None\n",
        "\n",
        "        # See third term of Equation (3).\n",
        "        commitment_loss = ((x - quantized_x.detach()) ** 2).mean()\n",
        "        # Straight-through gradient. See Section 3.2.\n",
        "        quantized_x = x + (quantized_x - x).detach()\n",
        "\n",
        "        if self.use_ema and self.training:\n",
        "            with torch.no_grad():\n",
        "                # See Appendix A.1 of \"Neural Discrete Representation Learning\".\n",
        "\n",
        "                # Cluster counts.\n",
        "                encoding_one_hots = F.one_hot(\n",
        "                    encoding_indices, self.num_embeddings\n",
        "                ).type(flat_x.dtype)\n",
        "                n_i_ts = encoding_one_hots.sum(0)\n",
        "                # Updated exponential moving average of the cluster counts.\n",
        "                # See Equation (6).\n",
        "                self.N_i_ts(n_i_ts)\n",
        "\n",
        "                # Exponential moving average of the embeddings. See Equation (7).\n",
        "                embed_sums = flat_x.transpose(0, 1) @ encoding_one_hots\n",
        "                self.m_i_ts(embed_sums)\n",
        "\n",
        "                # This is kind of weird.\n",
        "                # Compare: https://github.com/deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py#L270\n",
        "                # and Equation (8).\n",
        "                N_i_ts_sum = self.N_i_ts.average.sum()\n",
        "                N_i_ts_stable = (\n",
        "                    (self.N_i_ts.average + self.epsilon)\n",
        "                    / (N_i_ts_sum + self.num_embeddings * self.epsilon)\n",
        "                    * N_i_ts_sum\n",
        "                )\n",
        "                self.e_i_ts = self.m_i_ts.average / N_i_ts_stable.unsqueeze(0)\n",
        "\n",
        "        return (\n",
        "            quantized_x,\n",
        "            dictionary_loss,\n",
        "            commitment_loss,\n",
        "            encoding_indices.view(x.shape[0], -1),\n",
        "        )\n",
        "\n",
        "\n",
        "class VQVAE(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        num_hiddens,\n",
        "        num_downsampling_layers,\n",
        "        num_residual_layers,\n",
        "        num_residual_hiddens,\n",
        "        embedding_dim,\n",
        "        num_embeddings,\n",
        "        use_ema,\n",
        "        decay,\n",
        "        epsilon,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(\n",
        "            in_channels,\n",
        "            num_hiddens,\n",
        "            num_downsampling_layers,\n",
        "            num_residual_layers,\n",
        "            num_residual_hiddens,\n",
        "        )\n",
        "        self.pre_vq_conv = nn.Conv2d(\n",
        "            in_channels=num_hiddens, out_channels=embedding_dim, kernel_size=1\n",
        "        )\n",
        "        self.vq = VectorQuantizer(\n",
        "            embedding_dim, num_embeddings, use_ema, decay, epsilon\n",
        "        )\n",
        "        self.decoder = Decoder(\n",
        "            embedding_dim,\n",
        "            num_hiddens,\n",
        "            num_downsampling_layers,\n",
        "            num_residual_layers,\n",
        "            num_residual_hiddens,\n",
        "        )\n",
        "\n",
        "    def quantize(self, x):\n",
        "        z = self.pre_vq_conv(self.encoder(x))\n",
        "        (z_quantized, dictionary_loss, commitment_loss, encoding_indices) = self.vq(z)\n",
        "        return (z_quantized, dictionary_loss, commitment_loss, encoding_indices)\n",
        "\n",
        "    def forward(self, x):\n",
        "        (z_quantized, dictionary_loss, commitment_loss, _) = self.quantize(x)\n",
        "        x_recon = self.decoder(z_quantized)\n",
        "        return {\n",
        "            \"dictionary_loss\": dictionary_loss,\n",
        "            \"commitment_loss\": commitment_loss,\n",
        "            \"x_recon\": x_recon,\n",
        "        }"
      ],
      "metadata": {
        "id": "l1xWmyJ1jBkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See: https://github.com/deepmind/sonnet/blob/v2/examples/vqvae_example.ipynb.\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from PIL import Image\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "torch.set_printoptions(linewidth=160)\n",
        "\n",
        "\n",
        "def save_img_tensors_as_grid(img_tensors, nrows, f):\n",
        "    img_tensors = img_tensors.permute(0, 2, 3, 1)\n",
        "    imgs_array = img_tensors.detach().cpu().numpy()\n",
        "    imgs_array[imgs_array < -0.5] = -0.5\n",
        "    imgs_array[imgs_array > 0.5] = 0.5\n",
        "    imgs_array = 255 * (imgs_array + 0.5)\n",
        "    (batch_size, img_size) = img_tensors.shape[:2]\n",
        "    ncols = batch_size // nrows\n",
        "    img_arr = np.zeros((nrows * batch_size, ncols * batch_size, 3))\n",
        "    for idx in range(batch_size):\n",
        "        row_idx = idx // ncols\n",
        "        col_idx = idx % ncols\n",
        "        row_start = row_idx * img_size\n",
        "        row_end = row_start + img_size\n",
        "        col_start = col_idx * img_size\n",
        "        col_end = col_start + img_size\n",
        "        img_arr[row_start:row_end, col_start:col_end] = imgs_array[idx]\n",
        "\n",
        "    Image.fromarray(img_arr.astype(np.uint8), \"RGB\").save(f\"{f}.jpg\")\n",
        "\n",
        "\n",
        "# Initialize model.\n",
        "device = torch.device(\"cuda:0\")\n",
        "use_ema = True\n",
        "model_args = {\n",
        "    \"in_channels\": 3,\n",
        "    \"num_hiddens\": 128,\n",
        "    \"num_downsampling_layers\": 2,\n",
        "    \"num_residual_layers\": 2,\n",
        "    \"num_residual_hiddens\": 32,\n",
        "    \"embedding_dim\": 64,\n",
        "    \"num_embeddings\": 512,\n",
        "    \"use_ema\": use_ema,\n",
        "    \"decay\": 0.99,\n",
        "    \"epsilon\": 1e-5,\n",
        "}\n",
        "model = VQVAE(**model_args).to(device)\n",
        "\n",
        "# Initialize dataset.\n",
        "batch_size = 32\n",
        "workers = 10\n",
        "normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[1.0, 1.0, 1.0])\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ]\n",
        ")\n",
        "data_root = \"../data\"\n",
        "train_dataset = CIFAR10(data_root, True, transform, download=True)\n",
        "train_data_variance = np.var(train_dataset.data / 255)\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=workers,\n",
        ")\n",
        "\n",
        "# Multiplier for commitment loss. See Equation (3) in \"Neural Discrete Representation\n",
        "# Learning\".\n",
        "beta = 0.25\n",
        "\n",
        "# Initialize optimizer.\n",
        "train_params = [params for params in model.parameters()]\n",
        "lr = 3e-4\n",
        "optimizer = optim.Adam(train_params, lr=lr)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Train model.\n",
        "epochs = 7\n",
        "eval_every = 100\n",
        "best_train_loss = float(\"inf\")\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    total_train_loss = 0\n",
        "    total_recon_error = 0\n",
        "    n_train = 0\n",
        "    for (batch_idx, train_tensors) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        imgs = train_tensors[0].to(device)\n",
        "        out = model(imgs)\n",
        "        recon_error = criterion(out[\"x_recon\"], imgs) / train_data_variance\n",
        "        total_recon_error += recon_error.item()\n",
        "        loss = recon_error + beta * out[\"commitment_loss\"]\n",
        "        if not use_ema:\n",
        "            loss += out[\"dictionary_loss\"]\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        n_train += 1\n",
        "\n",
        "        if ((batch_idx + 1) % eval_every) == 0:\n",
        "            print(f\"epoch: {epoch}\\nbatch_idx: {batch_idx + 1}\", flush=True)\n",
        "            total_train_loss /= n_train\n",
        "            if total_train_loss < best_train_loss:\n",
        "                best_train_loss = total_train_loss\n",
        "\n",
        "            print(f\"total_train_loss: {total_train_loss}\")\n",
        "            print(f\"best_train_loss: {best_train_loss}\")\n",
        "            print(f\"recon_error: {total_recon_error / n_train}\\n\")\n",
        "\n",
        "            total_train_loss = 0\n",
        "            total_recon_error = 0\n",
        "            n_train = 0\n",
        "\n",
        "# Generate and save reconstructions.\n",
        "model.eval()\n",
        "\n",
        "valid_dataset = CIFAR10(data_root, False, transform, download=True)\n",
        "valid_loader = DataLoader(\n",
        "    dataset=valid_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=workers,\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for valid_tensors in valid_loader:\n",
        "        break\n",
        "\n",
        "    save_img_tensors_as_grid(valid_tensors[0], 4, \"true\")\n",
        "    save_img_tensors_as_grid(model(valid_tensors[0].to(device))[\"x_recon\"], 4, \"recon\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvutPKA0jCSS",
        "outputId": "6c1afd74-8699-4207-de4a-90732eb6036e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 43.7MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0\n",
            "batch_idx: 100\n",
            "total_train_loss: 0.6619812095165253\n",
            "best_train_loss: 0.6619812095165253\n",
            "recon_error: 0.621974071264267\n",
            "\n",
            "epoch: 0\n",
            "batch_idx: 200\n",
            "total_train_loss: 0.24854657918214798\n",
            "best_train_loss: 0.24854657918214798\n",
            "recon_error: 0.2360302287340164\n",
            "\n",
            "epoch: 0\n",
            "batch_idx: 300\n",
            "total_train_loss: 0.21815455675125123\n",
            "best_train_loss: 0.21815455675125123\n",
            "recon_error: 0.18964922159910202\n",
            "\n",
            "epoch: 0\n",
            "batch_idx: 400\n",
            "total_train_loss: 0.20056652531027794\n",
            "best_train_loss: 0.20056652531027794\n",
            "recon_error: 0.16867336213588716\n",
            "\n",
            "epoch: 0\n",
            "batch_idx: 500\n",
            "total_train_loss: 0.18440277457237245\n",
            "best_train_loss: 0.18440277457237245\n",
            "recon_error: 0.1502241588383913\n",
            "\n",
            "epoch: 0\n",
            "batch_idx: 600\n",
            "total_train_loss: 0.1686367480456829\n",
            "best_train_loss: 0.1686367480456829\n",
            "recon_error: 0.13522010058164596\n",
            "\n",
            "epoch: 0\n",
            "batch_idx: 700\n",
            "total_train_loss: 0.1585659761726856\n",
            "best_train_loss: 0.1585659761726856\n",
            "recon_error: 0.12681731417775155\n",
            "\n",
            "epoch: 0\n",
            "batch_idx: 800\n",
            "total_train_loss: 0.14370902977883815\n",
            "best_train_loss: 0.14370902977883815\n",
            "recon_error: 0.11596429996192455\n",
            "\n",
            "epoch: 0\n",
            "batch_idx: 900\n",
            "total_train_loss: 0.1392850022763014\n",
            "best_train_loss: 0.1392850022763014\n",
            "recon_error: 0.11198643490672111\n",
            "\n",
            "epoch: 0\n",
            "batch_idx: 1000\n",
            "total_train_loss: 0.13729452304542064\n",
            "best_train_loss: 0.13729452304542064\n",
            "recon_error: 0.10979652903974056\n",
            "\n",
            "epoch: 0\n",
            "batch_idx: 1100\n",
            "total_train_loss: 0.13387518867850304\n",
            "best_train_loss: 0.13387518867850304\n",
            "recon_error: 0.10399423234164715\n",
            "\n",
            "epoch: 0\n",
            "batch_idx: 1200\n",
            "total_train_loss: 0.1345122268795967\n",
            "best_train_loss: 0.13387518867850304\n",
            "recon_error: 0.10162480682134628\n",
            "\n",
            "epoch: 0\n",
            "batch_idx: 1300\n",
            "total_train_loss: 0.13232562884688379\n",
            "best_train_loss: 0.13232562884688379\n",
            "recon_error: 0.09744852654635906\n",
            "\n",
            "epoch: 0\n",
            "batch_idx: 1400\n",
            "total_train_loss: 0.13078663632273674\n",
            "best_train_loss: 0.13078663632273674\n",
            "recon_error: 0.09445348136126995\n",
            "\n",
            "epoch: 0\n",
            "batch_idx: 1500\n",
            "total_train_loss: 0.12845298759639262\n",
            "best_train_loss: 0.12845298759639262\n",
            "recon_error: 0.09087065890431405\n",
            "\n",
            "epoch: 1\n",
            "batch_idx: 100\n",
            "total_train_loss: 0.12345525249838829\n",
            "best_train_loss: 0.12345525249838829\n",
            "recon_error: 0.08622480466961861\n",
            "\n",
            "epoch: 1\n",
            "batch_idx: 200\n",
            "total_train_loss: 0.12259270079433918\n",
            "best_train_loss: 0.12259270079433918\n",
            "recon_error: 0.08557105258107185\n",
            "\n",
            "epoch: 1\n",
            "batch_idx: 300\n",
            "total_train_loss: 0.1193149222433567\n",
            "best_train_loss: 0.1193149222433567\n",
            "recon_error: 0.08315291658043861\n",
            "\n",
            "epoch: 1\n",
            "batch_idx: 400\n",
            "total_train_loss: 0.11762468807399273\n",
            "best_train_loss: 0.11762468807399273\n",
            "recon_error: 0.08129186131060123\n",
            "\n",
            "epoch: 1\n",
            "batch_idx: 500\n",
            "total_train_loss: 0.11418804310262204\n",
            "best_train_loss: 0.11418804310262204\n",
            "recon_error: 0.0793853336572647\n",
            "\n",
            "epoch: 1\n",
            "batch_idx: 600\n",
            "total_train_loss: 0.11289532601833344\n",
            "best_train_loss: 0.11289532601833344\n",
            "recon_error: 0.07809253741055727\n",
            "\n",
            "epoch: 1\n",
            "batch_idx: 700\n",
            "total_train_loss: 0.11355124451220036\n",
            "best_train_loss: 0.11289532601833344\n",
            "recon_error: 0.07845894150435924\n",
            "\n",
            "epoch: 1\n",
            "batch_idx: 800\n",
            "total_train_loss: 0.11142823055386543\n",
            "best_train_loss: 0.11142823055386543\n",
            "recon_error: 0.07605759866535663\n",
            "\n",
            "epoch: 1\n",
            "batch_idx: 900\n",
            "total_train_loss: 0.11040056310594082\n",
            "best_train_loss: 0.11040056310594082\n",
            "recon_error: 0.07464223183691501\n",
            "\n",
            "epoch: 1\n",
            "batch_idx: 1000\n",
            "total_train_loss: 0.10947383515536785\n",
            "best_train_loss: 0.10947383515536785\n",
            "recon_error: 0.07398463796824217\n",
            "\n",
            "epoch: 1\n",
            "batch_idx: 1100\n",
            "total_train_loss: 0.1089862509816885\n",
            "best_train_loss: 0.1089862509816885\n",
            "recon_error: 0.07337887033820152\n",
            "\n",
            "epoch: 1\n",
            "batch_idx: 1200\n",
            "total_train_loss: 0.10731664881110191\n",
            "best_train_loss: 0.10731664881110191\n",
            "recon_error: 0.07214692305773497\n",
            "\n",
            "epoch: 1\n",
            "batch_idx: 1300\n",
            "total_train_loss: 0.10627344653010368\n",
            "best_train_loss: 0.10627344653010368\n",
            "recon_error: 0.0711842706426978\n",
            "\n",
            "epoch: 1\n",
            "batch_idx: 1400\n",
            "total_train_loss: 0.10669534653425217\n",
            "best_train_loss: 0.10627344653010368\n",
            "recon_error: 0.07152638878673315\n",
            "\n",
            "epoch: 1\n",
            "batch_idx: 1500\n",
            "total_train_loss: 0.10558842048048973\n",
            "best_train_loss: 0.10558842048048973\n",
            "recon_error: 0.07042043916881084\n",
            "\n",
            "epoch: 2\n",
            "batch_idx: 100\n",
            "total_train_loss: 0.10480375319719315\n",
            "best_train_loss: 0.10480375319719315\n",
            "recon_error: 0.06949608825147152\n",
            "\n",
            "epoch: 2\n",
            "batch_idx: 200\n",
            "total_train_loss: 0.10352704830467702\n",
            "best_train_loss: 0.10352704830467702\n",
            "recon_error: 0.06821299977600574\n",
            "\n",
            "epoch: 2\n",
            "batch_idx: 300\n",
            "total_train_loss: 0.10270216807723045\n",
            "best_train_loss: 0.10270216807723045\n",
            "recon_error: 0.06758977193385363\n",
            "\n",
            "epoch: 2\n",
            "batch_idx: 400\n",
            "total_train_loss: 0.1023351251333952\n",
            "best_train_loss: 0.1023351251333952\n",
            "recon_error: 0.0677324652671814\n",
            "\n",
            "epoch: 2\n",
            "batch_idx: 500\n",
            "total_train_loss: 0.10084759682416916\n",
            "best_train_loss: 0.10084759682416916\n",
            "recon_error: 0.06627484794706107\n",
            "\n",
            "epoch: 2\n",
            "batch_idx: 600\n",
            "total_train_loss: 0.10176514804363251\n",
            "best_train_loss: 0.10084759682416916\n",
            "recon_error: 0.06657864801585674\n",
            "\n",
            "epoch: 2\n",
            "batch_idx: 700\n",
            "total_train_loss: 0.10142715841531753\n",
            "best_train_loss: 0.10084759682416916\n",
            "recon_error: 0.0667356513813138\n",
            "\n",
            "epoch: 2\n",
            "batch_idx: 800\n",
            "total_train_loss: 0.09945322312414646\n",
            "best_train_loss: 0.09945322312414646\n",
            "recon_error: 0.06452118847519159\n",
            "\n",
            "epoch: 2\n",
            "batch_idx: 900\n",
            "total_train_loss: 0.09925462432205677\n",
            "best_train_loss: 0.09925462432205677\n",
            "recon_error: 0.064188795350492\n",
            "\n",
            "epoch: 2\n",
            "batch_idx: 1000\n",
            "total_train_loss: 0.09859609924256801\n",
            "best_train_loss: 0.09859609924256801\n",
            "recon_error: 0.06343609247356653\n",
            "\n",
            "epoch: 2\n",
            "batch_idx: 1100\n",
            "total_train_loss: 0.09977720275521279\n",
            "best_train_loss: 0.09859609924256801\n",
            "recon_error: 0.06370573852211237\n",
            "\n",
            "epoch: 2\n",
            "batch_idx: 1200\n",
            "total_train_loss: 0.10049627289175987\n",
            "best_train_loss: 0.09859609924256801\n",
            "recon_error: 0.06414271898567676\n",
            "\n",
            "epoch: 2\n",
            "batch_idx: 1300\n",
            "total_train_loss: 0.09795906879007817\n",
            "best_train_loss: 0.09795906879007817\n",
            "recon_error: 0.062075874507427214\n",
            "\n",
            "epoch: 2\n",
            "batch_idx: 1400\n",
            "total_train_loss: 0.0986063714325428\n",
            "best_train_loss: 0.09795906879007817\n",
            "recon_error: 0.06231706451624632\n",
            "\n",
            "epoch: 2\n",
            "batch_idx: 1500\n",
            "total_train_loss: 0.09947397753596306\n",
            "best_train_loss: 0.09795906879007817\n",
            "recon_error: 0.06254754066467286\n",
            "\n",
            "epoch: 3\n",
            "batch_idx: 100\n",
            "total_train_loss: 0.09867378436028958\n",
            "best_train_loss: 0.09795906879007817\n",
            "recon_error: 0.06161080323159695\n",
            "\n",
            "epoch: 3\n",
            "batch_idx: 200\n",
            "total_train_loss: 0.09888218142092228\n",
            "best_train_loss: 0.09795906879007817\n",
            "recon_error: 0.06192922197282314\n",
            "\n",
            "epoch: 3\n",
            "batch_idx: 300\n",
            "total_train_loss: 0.09912592679262161\n",
            "best_train_loss: 0.09795906879007817\n",
            "recon_error: 0.06186739794909954\n",
            "\n",
            "epoch: 3\n",
            "batch_idx: 400\n",
            "total_train_loss: 0.09884500801563263\n",
            "best_train_loss: 0.09795906879007817\n",
            "recon_error: 0.06176153752952814\n",
            "\n",
            "epoch: 3\n",
            "batch_idx: 500\n",
            "total_train_loss: 0.09716638028621674\n",
            "best_train_loss: 0.09716638028621674\n",
            "recon_error: 0.060191707350313664\n",
            "\n",
            "epoch: 3\n",
            "batch_idx: 600\n",
            "total_train_loss: 0.0990404286980629\n",
            "best_train_loss: 0.09716638028621674\n",
            "recon_error: 0.0615768937394023\n",
            "\n",
            "epoch: 3\n",
            "batch_idx: 700\n",
            "total_train_loss: 0.09725172117352486\n",
            "best_train_loss: 0.09716638028621674\n",
            "recon_error: 0.06012023095041513\n",
            "\n",
            "epoch: 3\n",
            "batch_idx: 800\n",
            "total_train_loss: 0.09831631936132908\n",
            "best_train_loss: 0.09716638028621674\n",
            "recon_error: 0.06105624381452799\n",
            "\n",
            "epoch: 3\n",
            "batch_idx: 900\n",
            "total_train_loss: 0.09798858843743802\n",
            "best_train_loss: 0.09716638028621674\n",
            "recon_error: 0.06055804692208767\n",
            "\n",
            "epoch: 3\n",
            "batch_idx: 1000\n",
            "total_train_loss: 0.09623444348573684\n",
            "best_train_loss: 0.09623444348573684\n",
            "recon_error: 0.05922353558242321\n",
            "\n",
            "epoch: 3\n",
            "batch_idx: 1100\n",
            "total_train_loss: 0.09845639519393444\n",
            "best_train_loss: 0.09623444348573684\n",
            "recon_error: 0.06039179619401693\n",
            "\n",
            "epoch: 3\n",
            "batch_idx: 1200\n",
            "total_train_loss: 0.09695609897375107\n",
            "best_train_loss: 0.09623444348573684\n",
            "recon_error: 0.059458942897617814\n",
            "\n",
            "epoch: 3\n",
            "batch_idx: 1300\n",
            "total_train_loss: 0.09658515200018883\n",
            "best_train_loss: 0.09623444348573684\n",
            "recon_error: 0.05907931670546532\n",
            "\n",
            "epoch: 3\n",
            "batch_idx: 1400\n",
            "total_train_loss: 0.09813256464898586\n",
            "best_train_loss: 0.09623444348573684\n",
            "recon_error: 0.059978618435561654\n",
            "\n",
            "epoch: 3\n",
            "batch_idx: 1500\n",
            "total_train_loss: 0.0976903960108757\n",
            "best_train_loss: 0.09623444348573684\n",
            "recon_error: 0.059723284617066386\n",
            "\n",
            "epoch: 4\n",
            "batch_idx: 100\n",
            "total_train_loss: 0.09708384729921818\n",
            "best_train_loss: 0.09623444348573684\n",
            "recon_error: 0.05908897403627634\n",
            "\n",
            "epoch: 4\n",
            "batch_idx: 200\n",
            "total_train_loss: 0.09588823266327381\n",
            "best_train_loss: 0.09588823266327381\n",
            "recon_error: 0.05836590815335512\n",
            "\n",
            "epoch: 4\n",
            "batch_idx: 300\n",
            "total_train_loss: 0.09683236233890057\n",
            "best_train_loss: 0.09588823266327381\n",
            "recon_error: 0.05871105339378119\n",
            "\n",
            "epoch: 4\n",
            "batch_idx: 400\n",
            "total_train_loss: 0.09768213368952275\n",
            "best_train_loss: 0.09588823266327381\n",
            "recon_error: 0.059303555376827716\n",
            "\n",
            "epoch: 4\n",
            "batch_idx: 500\n",
            "total_train_loss: 0.09801731869578362\n",
            "best_train_loss: 0.09588823266327381\n",
            "recon_error: 0.05946186125278473\n",
            "\n",
            "epoch: 4\n",
            "batch_idx: 600\n",
            "total_train_loss: 0.09639774434268475\n",
            "best_train_loss: 0.09588823266327381\n",
            "recon_error: 0.058115372881293294\n",
            "\n",
            "epoch: 4\n",
            "batch_idx: 700\n",
            "total_train_loss: 0.0972396318614483\n",
            "best_train_loss: 0.09588823266327381\n",
            "recon_error: 0.05866461824625731\n",
            "\n",
            "epoch: 4\n",
            "batch_idx: 800\n",
            "total_train_loss: 0.09710799343883991\n",
            "best_train_loss: 0.09588823266327381\n",
            "recon_error: 0.05861267179250717\n",
            "\n",
            "epoch: 4\n",
            "batch_idx: 900\n",
            "total_train_loss: 0.0994066408276558\n",
            "best_train_loss: 0.09588823266327381\n",
            "recon_error: 0.05984502501785755\n",
            "\n",
            "epoch: 4\n",
            "batch_idx: 1000\n",
            "total_train_loss: 0.09689981892704963\n",
            "best_train_loss: 0.09588823266327381\n",
            "recon_error: 0.05830645628273487\n",
            "\n",
            "epoch: 4\n",
            "batch_idx: 1100\n",
            "total_train_loss: 0.09843875057995319\n",
            "best_train_loss: 0.09588823266327381\n",
            "recon_error: 0.05894566629081965\n",
            "\n",
            "epoch: 4\n",
            "batch_idx: 1200\n",
            "total_train_loss: 0.09793837115168572\n",
            "best_train_loss: 0.09588823266327381\n",
            "recon_error: 0.05905856125056744\n",
            "\n",
            "epoch: 4\n",
            "batch_idx: 1300\n",
            "total_train_loss: 0.09635759264230728\n",
            "best_train_loss: 0.09588823266327381\n",
            "recon_error: 0.05769579563289881\n",
            "\n",
            "epoch: 4\n",
            "batch_idx: 1400\n",
            "total_train_loss: 0.09560657843947411\n",
            "best_train_loss: 0.09560657843947411\n",
            "recon_error: 0.05719882387667894\n",
            "\n",
            "epoch: 4\n",
            "batch_idx: 1500\n",
            "total_train_loss: 0.09631551213562489\n",
            "best_train_loss: 0.09560657843947411\n",
            "recon_error: 0.05750191729515791\n",
            "\n",
            "epoch: 5\n",
            "batch_idx: 100\n",
            "total_train_loss: 0.09673948973417282\n",
            "best_train_loss: 0.09560657843947411\n",
            "recon_error: 0.057689258232712745\n",
            "\n",
            "epoch: 5\n",
            "batch_idx: 200\n",
            "total_train_loss: 0.09736633099615574\n",
            "best_train_loss: 0.09560657843947411\n",
            "recon_error: 0.058081450313329695\n",
            "\n",
            "epoch: 5\n",
            "batch_idx: 300\n",
            "total_train_loss: 0.09625446386635303\n",
            "best_train_loss: 0.09560657843947411\n",
            "recon_error: 0.05721873629838228\n",
            "\n",
            "epoch: 5\n",
            "batch_idx: 400\n",
            "total_train_loss: 0.09666262798011303\n",
            "best_train_loss: 0.09560657843947411\n",
            "recon_error: 0.05757782060652971\n",
            "\n",
            "epoch: 5\n",
            "batch_idx: 500\n",
            "total_train_loss: 0.09697767168283462\n",
            "best_train_loss: 0.09560657843947411\n",
            "recon_error: 0.057867517061531544\n",
            "\n",
            "epoch: 5\n",
            "batch_idx: 600\n",
            "total_train_loss: 0.09691127076745033\n",
            "best_train_loss: 0.09560657843947411\n",
            "recon_error: 0.05784785639494658\n",
            "\n",
            "epoch: 5\n",
            "batch_idx: 700\n",
            "total_train_loss: 0.09617803059518337\n",
            "best_train_loss: 0.09560657843947411\n",
            "recon_error: 0.05741434372961521\n",
            "\n",
            "epoch: 5\n",
            "batch_idx: 800\n",
            "total_train_loss: 0.09571655951440335\n",
            "best_train_loss: 0.09560657843947411\n",
            "recon_error: 0.05674839675426483\n",
            "\n",
            "epoch: 5\n",
            "batch_idx: 900\n",
            "total_train_loss: 0.0977975294739008\n",
            "best_train_loss: 0.09560657843947411\n",
            "recon_error: 0.058217576183378694\n",
            "\n",
            "epoch: 5\n",
            "batch_idx: 1000\n",
            "total_train_loss: 0.09806520976126194\n",
            "best_train_loss: 0.09560657843947411\n",
            "recon_error: 0.05833434656262398\n",
            "\n",
            "epoch: 5\n",
            "batch_idx: 1100\n",
            "total_train_loss: 0.09635073013603687\n",
            "best_train_loss: 0.09560657843947411\n",
            "recon_error: 0.05713427498936653\n",
            "\n",
            "epoch: 5\n",
            "batch_idx: 1200\n",
            "total_train_loss: 0.09746463872492313\n",
            "best_train_loss: 0.09560657843947411\n",
            "recon_error: 0.0578700615093112\n",
            "\n",
            "epoch: 5\n",
            "batch_idx: 1300\n",
            "total_train_loss: 0.0958137546479702\n",
            "best_train_loss: 0.09560657843947411\n",
            "recon_error: 0.05680815279483795\n",
            "\n",
            "epoch: 5\n",
            "batch_idx: 1400\n",
            "total_train_loss: 0.09691378735005855\n",
            "best_train_loss: 0.09560657843947411\n",
            "recon_error: 0.057421513237059116\n",
            "\n",
            "epoch: 5\n",
            "batch_idx: 1500\n",
            "total_train_loss: 0.09632888071238994\n",
            "best_train_loss: 0.09560657843947411\n",
            "recon_error: 0.05695303808897734\n",
            "\n",
            "epoch: 6\n",
            "batch_idx: 100\n",
            "total_train_loss: 0.09745007768273353\n",
            "best_train_loss: 0.09560657843947411\n",
            "recon_error: 0.05761481359601021\n",
            "\n",
            "epoch: 6\n",
            "batch_idx: 200\n",
            "total_train_loss: 0.09600868888199329\n",
            "best_train_loss: 0.09560657843947411\n",
            "recon_error: 0.056636129170656205\n",
            "\n",
            "epoch: 6\n",
            "batch_idx: 300\n",
            "total_train_loss: 0.09631695985794067\n",
            "best_train_loss: 0.09560657843947411\n",
            "recon_error: 0.05716144863516092\n",
            "\n",
            "epoch: 6\n",
            "batch_idx: 400\n",
            "total_train_loss: 0.09606496907770634\n",
            "best_train_loss: 0.09560657843947411\n",
            "recon_error: 0.05676396910101175\n",
            "\n",
            "epoch: 6\n",
            "batch_idx: 500\n",
            "total_train_loss: 0.0966863264888525\n",
            "best_train_loss: 0.09560657843947411\n",
            "recon_error: 0.05730476509779692\n",
            "\n",
            "epoch: 6\n",
            "batch_idx: 600\n",
            "total_train_loss: 0.09623326353728771\n",
            "best_train_loss: 0.09560657843947411\n",
            "recon_error: 0.05670953720808029\n",
            "\n",
            "epoch: 6\n",
            "batch_idx: 700\n",
            "total_train_loss: 0.09567044205963611\n",
            "best_train_loss: 0.09560657843947411\n",
            "recon_error: 0.05629000492393971\n",
            "\n",
            "epoch: 6\n",
            "batch_idx: 800\n",
            "total_train_loss: 0.09581826694309711\n",
            "best_train_loss: 0.09560657843947411\n",
            "recon_error: 0.05650453191250562\n",
            "\n",
            "epoch: 6\n",
            "batch_idx: 900\n",
            "total_train_loss: 0.09525645069777966\n",
            "best_train_loss: 0.09525645069777966\n",
            "recon_error: 0.05615080378949642\n",
            "\n",
            "epoch: 6\n",
            "batch_idx: 1000\n",
            "total_train_loss: 0.09543374188244343\n",
            "best_train_loss: 0.09525645069777966\n",
            "recon_error: 0.05632748350501061\n",
            "\n",
            "epoch: 6\n",
            "batch_idx: 1100\n",
            "total_train_loss: 0.0959787517040968\n",
            "best_train_loss: 0.09525645069777966\n",
            "recon_error: 0.056519807055592536\n",
            "\n",
            "epoch: 6\n",
            "batch_idx: 1200\n",
            "total_train_loss: 0.09626045063138008\n",
            "best_train_loss: 0.09525645069777966\n",
            "recon_error: 0.05683562528342009\n",
            "\n",
            "epoch: 6\n",
            "batch_idx: 1300\n",
            "total_train_loss: 0.09635112442076206\n",
            "best_train_loss: 0.09525645069777966\n",
            "recon_error: 0.05658860296010971\n",
            "\n",
            "epoch: 6\n",
            "batch_idx: 1400\n",
            "total_train_loss: 0.0963639872521162\n",
            "best_train_loss: 0.09525645069777966\n",
            "recon_error: 0.05655186466872692\n",
            "\n",
            "epoch: 6\n",
            "batch_idx: 1500\n",
            "total_train_loss: 0.09681681543588638\n",
            "best_train_loss: 0.09525645069777966\n",
            "recon_error: 0.05726499240845442\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W29CAoMPjJQ7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}